#!/usr/bin/env python3
"""
Setup para Gemma 3 y Qwen2.5-VL
Modelos de √∫ltima generaci√≥n para an√°lisis avanzado de im√°genes
"""

import os
import torch
from transformers import (
    AutoProcessor, AutoModelForCausalLM,
    Qwen2VLForConditionalGeneration, Qwen2VLProcessor,
    PaliGemmaProcessor, PaliGemmaForConditionalGeneration
)
from PIL import Image
import requests
from io import BytesIO

def setup_latest_models():
    """Configura Gemma 3 y Qwen2.5-VL para tu hardware"""
    print("üöÄ Configurando modelos de √öLTIMA GENERACI√ìN")
    print("=" * 65)
    print("üéØ Gemma 3 + Qwen2.5-VL para an√°lisis avanzado de figuras")
    print("üîß Optimizado para: AMD 6750XT + Ryzen 7 5700X + 32GB RAM")
    print("=" * 65)
    
    latest_models = [
        {
            "name": "Qwen2.5-VL-7B",
            "hf_name": "Qwen/Qwen2.5-VL-7B-Instruct",
            "processor": Qwen2VLProcessor,
            "model": Qwen2VLForConditionalGeneration,
            "dir": "./models/qwen25_vl_7b",
            "description": "Qwen2.5-VL - √öltima versi√≥n mejorada",
            "ram_needed": "12-18GB",
            "specialty": "An√°lisis multimodal avanzado con mejor comprensi√≥n",
            "release": "2024 - Estado del arte"
        },
        {
            "name": "Qwen2.5-VL-3B",
            "hf_name": "Qwen/Qwen2.5-VL-3B-Instruct", 
            "processor": Qwen2VLProcessor,
            "model": Qwen2VLForConditionalGeneration,
            "dir": "./models/qwen25_vl_3b",
            "description": "Qwen2.5-VL 3B - Versi√≥n m√°s eficiente",
            "ram_needed": "6-12GB",
            "specialty": "Balance perfecto entre velocidad y calidad",
            "release": "2024 - Optimizado"
        },
        {
            "name": "PaliGemma-3B",
            "hf_name": "google/paligemma-3b-pt-448",
            "processor": PaliGemmaProcessor,
            "model": PaliGemmaForConditionalGeneration,
            "dir": "./models/paligemma_3b",
            "description": "PaliGemma 3B - Basado en Gemma de Google",
            "ram_needed": "8-14GB",
            "specialty": "Excelente para an√°lisis detallado y comprensi√≥n visual",
            "release": "2024 - Google Research"
        },
        {
            "name": "PaliGemma-3B-Mix",
            "hf_name": "google/paligemma-3b-mix-448",
            "processor": PaliGemmaProcessor,
            "model": PaliGemmaForConditionalGeneration,
            "dir": "./models/paligemma_3b_mix",
            "description": "PaliGemma 3B Mix - Entrenamiento mejorado",
            "ram_needed": "8-14GB", 
            "specialty": "Versi√≥n optimizada para m√∫ltiples tareas visuales",
            "release": "2024 - Versi√≥n mejorada"
        },
        {
            "name": "Qwen2.5-VL-72B-AWQ",
            "hf_name": "Qwen/Qwen2.5-VL-72B-Instruct-AWQ",
            "processor": Qwen2VLProcessor,
            "model": Qwen2VLForConditionalGeneration,
            "dir": "./models/qwen25_vl_72b_awq",
            "description": "Qwen2.5-VL 72B AWQ - Modelo gigante cuantizado",
            "ram_needed": "24-32GB",
            "specialty": "M√°xima calidad posible con cuantizaci√≥n AWQ",
            "release": "2024 - Flagship model"
        }
    ]
    
    print("üéØ Modelos disponibles:")
    for i, model in enumerate(latest_models, 1):
        print(f"  {i}. {model['name']}")
        print(f"     üìã {model['description']}")
        print(f"     üé™ {model['specialty']}")
        print(f"     üíæ RAM: {model['ram_needed']}")
        print(f"     üìÖ {model['release']}")
        print()
    
    print("üí° Recomendaciones para tu caso:")
    print("   ü•á Qwen2.5-VL-7B - Mejor balance calidad/velocidad")
    print("   ü•à PaliGemma-3B-Mix - Excelente para figuras humanas")
    print("   ü•â Qwen2.5-VL-72B-AWQ - M√°xima calidad (usa toda tu RAM)")
    print()
    
    # Selecci√≥n interactiva
    choice = input("¬øQu√© modelo(s) prefieres? (1-5, m√∫ltiples separados por coma, o 'recommended'): ").strip()
    
    if choice.lower() == 'recommended':
        selected_models = [latest_models[0], latest_models[2]]  # Qwen2.5-VL-7B + PaliGemma
        print("‚ú® Seleccionados modelos recomendados: Qwen2.5-VL-7B + PaliGemma-3B-Mix")
    elif ',' in choice:
        indices = [int(x.strip()) - 1 for x in choice.split(',') if x.strip().isdigit()]
        selected_models = [latest_models[i] for i in indices if 0 <= i < len(latest_models)]
    elif choice.isdigit():
        idx = int(choice) - 1
        if 0 <= idx < len(latest_models):
            selected_models = [latest_models[idx]]
        else:
            print("‚ö†Ô∏è Selecci√≥n inv√°lida, usando Qwen2.5-VL-7B")
            selected_models = [latest_models[0]]
    else:
        print("‚ö†Ô∏è Selecci√≥n inv√°lida, usando modelos recomendados")
        selected_models = [latest_models[0], latest_models[2]]
    
    for model_config in selected_models:
        download_latest_model(model_config)

def download_latest_model(config):
    """Descarga un modelo de √∫ltima generaci√≥n"""
    print(f"\nüì• Descargando {config['name']}...")
    print(f"üìÅ Directorio: {config['dir']}")
    print(f"üéØ Especialidad: {config['specialty']}")
    print(f"‚è∞ Esto puede tardar 10-30 minutos dependiendo del modelo...")
    
    try:
        os.makedirs(config['dir'], exist_ok=True)
        
        # Configuraciones espec√≠ficas por modelo
        model_kwargs = {
            "torch_dtype": torch.float32,
            "low_cpu_mem_usage": False,  # Tienes 32GB
            "device_map": None,          # CPU
        }
        
        # Configuraciones especiales
        if "qwen" in config['name'].lower():
            print("  üîß Configurando Qwen2.5-VL...")
            model_kwargs.update({
                "trust_remote_code": True,
                "attn_implementation": "eager",  # Mejor compatibilidad CPU
            })
        elif "paligemma" in config['name'].lower() or "gemma" in config['name'].lower():
            print("  üîß Configurando PaliGemma...")
            model_kwargs.update({
                "revision": "bfloat16",  # Usar versi√≥n optimizada
            })
        
        # Descargar procesador
        print("  üì• Descargando procesador...")
        if "qwen" in config['name'].lower():
            processor = config['processor'].from_pretrained(
                config['hf_name'],
                trust_remote_code=True
            )
        else:
            processor = config['processor'].from_pretrained(config['hf_name'])
        
        # Descargar modelo
        print("  üì• Descargando modelo...")
        print("      ‚è≥ Este paso puede tardar bastante para modelos grandes...")
        
        if "qwen" in config['name'].lower():
            model = config['model'].from_pretrained(
                config['hf_name'],
                trust_remote_code=True,
                **model_kwargs
            )
        else:
            model = config['model'].from_pretrained(
                config['hf_name'],
                **model_kwargs
            )
        
        # Guardar localmente
        print("  üíæ Guardando modelo...")
        processor.save_pretrained(config['dir'])
        model.save_pretrained(config['dir'])
        
        # Crear configuraci√≥n
        create_latest_model_config(config)
        
        # Probar modelo
        test_latest_model(processor, model, config)
        
        print(f"  ‚úÖ {config['name']} instalado correctamente!")
        
    except Exception as e:
        print(f"  ‚ùå Error con {config['name']}: {str(e)}")
        print(f"  üí° Posibles soluciones:")
        print(f"     - Verificar conexi√≥n a internet")
        print(f"     - Actualizar transformers: pip install transformers>=4.44.0")
        print(f"     - Verificar espacio en disco disponible")
        
        # Intentar modelo alternativo m√°s peque√±o
        if "72b" in config['name'].lower():
            print(f"  üîÑ Modelo muy grande, prueba la versi√≥n 7B o 3B")
        elif "qwen" in config['name'].lower():
            print(f"  üîÑ Si falla, prueba: pip install qwen-vl")

def create_latest_model_config(config):
    """Crea archivo de configuraci√≥n para modelos nuevos"""
    config_content = f"""# Configuraci√≥n para {config['name']}
MODEL_NAME={config['name']}
HF_NAME={config['hf_name']}
MODEL_DIR={config['dir']}
SPECIALTY={config['specialty']}
RAM_NEEDED={config['ram_needed']}
RELEASE={config['release']}
PROCESSOR_CLASS={config['processor'].__name__}
MODEL_CLASS={config['model'].__name__}

# Configuraciones especiales
TRUST_REMOTE_CODE={"true" if "qwen" in config['name'].lower() else "false"}
ATTN_IMPLEMENTATION=eager
TORCH_DTYPE=float32
"""
    
    config_file = os.path.join(config['dir'], "model_info.txt")
    with open(config_file, "w") as f:
        f.write(config_content)
    
    print(f"  üìÑ Configuraci√≥n guardada en {config_file}")

def test_latest_model(processor, model, config):
    """Prueba los modelos de √∫ltima generaci√≥n"""
    try:
        print(f"  üß™ Probando {config['name']}...")
        
        # Crear imagen de prueba (evitar descargas que puedan fallar)
        image = Image.new('RGB', (448, 448), color=(100, 150, 200))
        print("    üñºÔ∏è Usando imagen de prueba generada")
        
        # Prompts espec√≠ficos seg√∫n el modelo
        if "qwen" in config['name'].lower():
            # Qwen2.5-VL usa formato de chat
            messages = [
                {
                    "role": "user", 
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": "Analyze this image and describe what you see, focusing on any people, their emotions, and the overall situation."}
                    ]
                }
            ]
            
            # Aplicar template de chat
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text], images=[image], return_tensors="pt")
            
        elif "paligemma" in config['name'].lower():
            # PaliGemma usa formato directo
            prompt = "describe en detalle"
            inputs = processor(text=prompt, images=image, return_tensors="pt")
        
        else:
            # Formato gen√©rico
            prompt = "Describe this image in detail"
            inputs = processor(text=prompt, images=image, return_tensors="pt")
        
        # Generar respuesta
        print("    ‚ö° Generando descripci√≥n de prueba...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=50,  # Limitado para la prueba
                num_beams=2,
                early_stopping=True,
                do_sample=False
            )
        
        # Decodificar seg√∫n el modelo
        if "qwen" in config['name'].lower():
            # Para Qwen, extraer solo la parte nueva
            input_len = inputs["input_ids"].shape[1]
            generated_ids = generated_ids[:, input_len:]
            description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        else:
            # Para otros modelos
            description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"    üìù Respuesta: {description[:100]}...")
        print(f"    ‚úÖ {config['name']} funcionando correctamente")
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è Error en prueba: {str(e)}")
        print(f"    üí° El modelo se instal√≥ pero la prueba fall√≥. Esto es normal para algunos modelos nuevos.")

def create_latest_prompts():
    """Crea prompts optimizados para Gemma 3 y Qwen2.5-VL"""
    latest_prompts = {
        # Prompts para Qwen2.5-VL (formato chat)
        "qwen25_figure_analysis": {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Analyze the main human figure in this image. Provide a detailed description of their physical appearance, emotional state, body language, clothing, and the situation they find themselves in. Pay attention to facial expressions, posture, and any contextual clues about their circumstances."}
            ]
        },
        
        "qwen25_emotional_deep": {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Focus deeply on the emotional and psychological aspects of the person in this image. What emotions are visible? What does their body language suggest about their mental state? What situation might have led to this emotional expression?"}
            ]
        },
        
        "qwen25_comprehensive": {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Provide the most comprehensive analysis possible of this image, focusing particularly on any human subjects. Include physical appearance, emotional state, activities, environment, clothing, facial expressions, body language, and any contextual information that helps understand the scene and the person's situation."}
            ]
        },
        
        # Prompts para PaliGemma (formato directo)
        "paligemma_detailed": "Describe en gran detalle esta imagen, enfoc√°ndote en las personas, sus emociones, actividades y el contexto de la situaci√≥n",
        
        "paligemma_figure": "Analiza la figura principal en esta imagen: apariencia f√≠sica, expresi√≥n facial, lenguaje corporal, ropa y situaci√≥n",
        
        "paligemma_emotional": "Describe el estado emocional y psicol√≥gico de la persona en esta imagen, incluyendo expresiones faciales y lenguaje corporal",
        
        # Prompts gen√©ricos optimizados para modelos nuevos
        "latest_comprehensive": "Provide an extremely detailed and comprehensive analysis of this image, with special focus on any human figures, their emotional states, activities, and the overall context of the scene.",
        
        "latest_figure_focus": "Focus specifically on analyzing any human figures in this image. Describe their appearance, emotions, actions, and the situation they appear to be in.",
        
        "latest_situational": "Analyze the situation depicted in this image. What is happening? Who are the people involved? What is the context and what might have led to this moment?"
    }
    
    # Guardar prompts
    with open("latest_prompts.py", "w", encoding="utf-8") as f:
        f.write("# Prompts optimizados para Gemma 3 y Qwen2.5-VL\n\n")
        f.write("import json\n\n")
        f.write("LATEST_PROMPTS = {\n")
        
        for key, prompt in latest_prompts.items():
            if isinstance(prompt, dict):
                # Para prompts de chat (Qwen)
                f.write(f'    "{key}": {json.dumps(prompt, ensure_ascii=False, indent=8)},\n\n')
            else:
                # Para prompts de texto directo
                f.write(f'    "{key}": """{prompt}""",\n\n')
        
        f.write("}\n\n")
        
        f.write("""
def get_qwen25_prompt(prompt_type, custom_text=""):
    \"\"\"Genera prompt para Qwen2.5-VL en formato chat\"\"\"
    base_prompts = {
        'figure': LATEST_PROMPTS['qwen25_figure_analysis'],
        'emotional': LATEST_PROMPTS['qwen25_emotional_deep'],
        'comprehensive': LATEST_PROMPTS['qwen25_comprehensive']
    }
    
    if custom_text:
        return {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": custom_text}
            ]
        }
    
    return base_prompts.get(prompt_type, base_prompts['figure'])

def get_paligemma_prompt(prompt_type, custom_text=""):
    \"\"\"Genera prompt para PaliGemma\"\"\"
    base_prompts = {
        'figure': LATEST_PROMPTS['paligemma_figure'],
        'emotional': LATEST_PROMPTS['paligemma_emotional'],
        'detailed': LATEST_PROMPTS['paligemma_detailed']
    }
    
    if custom_text:
        return custom_text
    
    return base_prompts.get(prompt_type, base_prompts['detailed'])

# Ejemplo de uso:
# from latest_prompts import get_qwen25_prompt, get_paligemma_prompt
# qwen_prompt = get_qwen25_prompt('figure')
# gemma_prompt = get_paligemma_prompt('emotional')
""")
    
    print("üìö Prompts optimizados guardados en 'latest_prompts.py'")

if __name__ == "__main__":
    print("üéØ Setup para MODELOS DE √öLTIMA GENERACI√ìN")
    print("üîß Gemma 3 (PaliGemma) + Qwen2.5-VL")
    print("üíª Hardware: AMD 6750XT + Ryzen 7 5700X + 32GB RAM")
    print("=" * 70)
    
    try:
        setup_latest_models()
        create_latest_prompts()
        
        print("\nüéâ ¬°Configuraci√≥n de modelos de √∫ltima generaci√≥n completada!")
        print("üìã Pr√≥ximos pasos:")
        print("  1. Usar server_latest.py para cargar estos modelos")
        print("  2. Probar con client_complete.py")
        print("  3. Experimentar con latest_prompts.py")
        
        print("\nüí° Modelos recomendados para an√°lisis de figuras:")
        print("  ü•á Qwen2.5-VL-7B - Estado del arte en comprensi√≥n visual")
        print("  ü•à PaliGemma-3B-Mix - Excelente de Google Research")
        print("  ü•â Qwen2.5-VL-72B-AWQ - M√°xima calidad posible")
        
        print("\n‚ö†Ô∏è Nota importante:")
        print("  Estos son modelos muy nuevos (2024). Si alguno falla:")
        print("  ‚Ä¢ Actualiza transformers: pip install transformers>=4.44.0")
        print("  ‚Ä¢ Verifica dependencias: pip install qwen-vl accelerate")
        
    except Exception as e:
        print(f"‚ùå Error durante la configuraci√≥n: {str(e)}")
        print("üí° Soluciones sugeridas:")
        print("  ‚Ä¢ pip install --upgrade transformers accelerate torch")
        print("  ‚Ä¢ Verificar conexi√≥n a internet estable")
        print("  ‚Ä¢ Comprobar espacio disponible en disco (>20GB)")