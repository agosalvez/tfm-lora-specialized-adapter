#!/usr/bin/env python3
"""
Setup para Gemma 3 y Qwen2.5-VL
Modelos de Ãºltima generaciÃ³n para anÃ¡lisis avanzado de imÃ¡genes
"""

import os
import torch
from transformers import (
    AutoProcessor, AutoModelForCausalLM,
    Qwen2VLForConditionalGeneration, Qwen2VLProcessor,
    PaliGemmaProcessor, PaliGemmaForConditionalGeneration
)
from PIL import Image
import requests
from io import BytesIO

def setup_latest_models():
    """Configura Gemma 3 y Qwen2.5-VL para tu hardware"""
    print("ðŸš€ Configurando modelos de ÃšLTIMA GENERACIÃ“N")
    print("=" * 65)
    print("ðŸŽ¯ Gemma 3 + Qwen2.5-VL para anÃ¡lisis avanzado de figuras")
    print("ðŸ”§ Optimizado para: AMD 6750XT + Ryzen 7 5700X + 32GB RAM")
    print("=" * 65)
    
    latest_models = [
        {
            "name": "Qwen2.5-VL-7B",
            "hf_name": "Qwen/Qwen2.5-VL-7B-Instruct",
            "processor": Qwen2VLProcessor,
            "model": Qwen2VLForConditionalGeneration,
            "dir": "./models/qwen25_vl_7b",
            "description": "Qwen2.5-VL - Ãšltima versiÃ³n mejorada",
            "ram_needed": "12-18GB",
            "specialty": "AnÃ¡lisis multimodal avanzado con mejor comprensiÃ³n",
            "release": "2024 - Estado del arte"
        },
        {
            "name": "Qwen2.5-VL-3B",
            "hf_name": "Qwen/Qwen2.5-VL-3B-Instruct", 
            "processor": Qwen2VLProcessor,
            "model": Qwen2VLForConditionalGeneration,
            "dir": "./models/qwen25_vl_3b",
            "description": "Qwen2.5-VL 3B - VersiÃ³n mÃ¡s eficiente",
            "ram_needed": "6-12GB",
            "specialty": "Balance perfecto entre velocidad y calidad",
            "release": "2024 - Optimizado"
        },
        {
            "name": "PaliGemma-3B",
            "hf_name": "google/paligemma-3b-pt-448",
            "processor": PaliGemmaProcessor,
            "model": PaliGemmaForConditionalGeneration,
            "dir": "./models/paligemma_3b",
            "description": "PaliGemma 3B - Basado en Gemma de Google",
            "ram_needed": "8-14GB",
            "specialty": "Excelente para anÃ¡lisis detallado y comprensiÃ³n visual",
            "release": "2024 - Google Research"
        },
        {
            "name": "PaliGemma-3B-Mix",
            "hf_name": "google/paligemma-3b-mix-448",
            "processor": PaliGemmaProcessor,
            "model": PaliGemmaForConditionalGeneration,
            "dir": "./models/paligemma_3b_mix",
            "description": "PaliGemma 3B Mix - Entrenamiento mejorado",
            "ram_needed": "8-14GB", 
            "specialty": "VersiÃ³n optimizada para mÃºltiples tareas visuales",
            "release": "2024 - VersiÃ³n mejorada"
        },
        {
            "name": "Qwen2.5-VL-72B-AWQ",
            "hf_name": "Qwen/Qwen2.5-VL-72B-Instruct-AWQ",
            "processor": Qwen2VLProcessor,
            "model": Qwen2VLForConditionalGeneration,
            "dir": "./models/qwen25_vl_72b_awq",
            "description": "Qwen2.5-VL 72B AWQ - Modelo gigante cuantizado",
            "ram_needed": "24-32GB",
            "specialty": "MÃ¡xima calidad posible con cuantizaciÃ³n AWQ",
            "release": "2024 - Flagship model"
        }
    ]
    
    print("ðŸŽ¯ Modelos disponibles:")
    for i, model in enumerate(latest_models, 1):
        print(f"  {i}. {model['name']}")
        print(f"     ðŸ“‹ {model['description']}")
        print(f"     ðŸŽª {model['specialty']}")
        print(f"     ðŸ’¾ RAM: {model['ram_needed']}")
        print(f"     ðŸ“… {model['release']}")
        print()
    
    print("ðŸ’¡ Recomendaciones para tu caso:")
    print("   ðŸ¥‡ Qwen2.5-VL-7B - Mejor balance calidad/velocidad")
    print("   ðŸ¥ˆ PaliGemma-3B-Mix - Excelente para figuras humanas")
    print("   ðŸ¥‰ Qwen2.5-VL-72B-AWQ - MÃ¡xima calidad (usa toda tu RAM)")
    print()
    
    # SelecciÃ³n interactiva
    choice = input("Â¿QuÃ© modelo(s) prefieres? (1-5, mÃºltiples separados por coma, o 'recommended'): ").strip()
    
    if choice.lower() == 'recommended':
        selected_models = [latest_models[0], latest_models[2]]  # Qwen2.5-VL-7B + PaliGemma
        print("âœ¨ Seleccionados modelos recomendados: Qwen2.5-VL-7B + PaliGemma-3B-Mix")
    elif ',' in choice:
        indices = [int(x.strip()) - 1 for x in choice.split(',') if x.strip().isdigit()]
        selected_models = [latest_models[i] for i in indices if 0 <= i < len(latest_models)]
    elif choice.isdigit():
        idx = int(choice) - 1
        if 0 <= idx < len(latest_models):
            selected_models = [latest_models[idx]]
        else:
            print("âš ï¸ SelecciÃ³n invÃ¡lida, usando Qwen2.5-VL-7B")
            selected_models = [latest_models[0]]
    else:
        print("âš ï¸ SelecciÃ³n invÃ¡lida, usando modelos recomendados")
        selected_models = [latest_models[0], latest_models[2]]
    
    for model_config in selected_models:
        download_latest_model(model_config)

def download_latest_model(config):
    """Descarga un modelo de Ãºltima generaciÃ³n"""
    print(f"\nðŸ“¥ Descargando {config['name']}...")
    print(f"ðŸ“ Directorio: {config['dir']}")
    print(f"ðŸŽ¯ Especialidad: {config['specialty']}")
    print(f"â° Esto puede tardar 10-30 minutos dependiendo del modelo...")
    
    try:
        os.makedirs(config['dir'], exist_ok=True)
        
        # Configuraciones especÃ­ficas por modelo
        model_kwargs = {
            "torch_dtype": torch.float32,
            "low_cpu_mem_usage": False,  # Tienes 32GB
            "device_map": None,          # CPU
        }
        
        # Configuraciones especiales
        if "qwen" in config['name'].lower():
            print("  ðŸ”§ Configurando Qwen2.5-VL...")
            model_kwargs.update({
                "trust_remote_code": True,
                "attn_implementation": "eager",  # Mejor compatibilidad CPU
            })
        elif "paligemma" in config['name'].lower() or "gemma" in config['name'].lower():
            print("  ðŸ”§ Configurando PaliGemma...")
            model_kwargs.update({
                "revision": "bfloat16",  # Usar versiÃ³n optimizada
            })
        
        # Descargar procesador
        print("  ðŸ“¥ Descargando procesador...")
        if "qwen" in config['name'].lower():
            processor = config['processor'].from_pretrained(
                config['hf_name'],
                trust_remote_code=True
            )
        else:
            processor = config['processor'].from_pretrained(config['hf_name'])
        
        # Descargar modelo
        print("  ðŸ“¥ Descargando modelo...")
        print("      â³ Este paso puede tardar bastante para modelos grandes...")
        
        if "qwen" in config['name'].lower():
            model = config['model'].from_pretrained(
                config['hf_name'],
                trust_remote_code=True,
                **model_kwargs
            )
        else:
            model = config['model'].from_pretrained(
                config['hf_name'],
                **model_kwargs
            )
        
        # Guardar localmente
        print("  ðŸ’¾ Guardando modelo...")
        processor.save_pretrained(config['dir'])
        model.save_pretrained(config['dir'])
        
        # Crear configuraciÃ³n
        create_latest_model_config(config)
        
        # Probar modelo
        test_latest_model(processor, model, config)
        
        print(f"  âœ… {config['name']} instalado correctamente!")
        
    except Exception as e:
        print(f"  âŒ Error con {config['name']}: {str(e)}")
        print(f"  ðŸ’¡ Posibles soluciones:")
        print(f"     - Verificar conexiÃ³n a internet")
        print(f"     - Actualizar transformers: pip install transformers>=4.44.0")
        print(f"     - Verificar espacio en disco disponible")
        
        # Intentar modelo alternativo mÃ¡s pequeÃ±o
        if "72b" in config['name'].lower():
            print(f"  ðŸ”„ Modelo muy grande, prueba la versiÃ³n 7B o 3B")
        elif "qwen" in config['name'].lower():
            print(f"  ðŸ”„ Si falla, prueba: pip install qwen-vl")

def create_latest_model_config(config):
    """Crea archivo de configuraciÃ³n para modelos nuevos"""
    config_content = f"""# ConfiguraciÃ³n para {config['name']}
MODEL_NAME={config['name']}
HF_NAME={config['hf_name']}
MODEL_DIR={config['dir']}
SPECIALTY={config['specialty']}
RAM_NEEDED={config['ram_needed']}
RELEASE={config['release']}
PROCESSOR_CLASS={config['processor'].__name__}
MODEL_CLASS={config['model'].__name__}

# Configuraciones especiales
TRUST_REMOTE_CODE={"true" if "qwen" in config['name'].lower() else "false"}
ATTN_IMPLEMENTATION=eager
TORCH_DTYPE=float32
"""
    
    config_file = os.path.join(config['dir'], "model_info.txt")
    with open(config_file, "w") as f:
        f.write(config_content)
    
    print(f"  ðŸ“„ ConfiguraciÃ³n guardada en {config_file}")

def test_latest_model(processor, model, config):
    """Prueba los modelos de Ãºltima generaciÃ³n"""
    try:
        print(f"  ðŸ§ª Probando {config['name']}...")
        
        # Crear imagen de prueba (evitar descargas que puedan fallar)
        image = Image.new('RGB', (448, 448), color=(100, 150, 200))
        print("    ðŸ–¼ï¸ Usando imagen de prueba generada")
        
        # Prompts especÃ­ficos segÃºn el modelo
        if "qwen" in config['name'].lower():
            # Qwen2.5-VL usa formato de chat
            messages = [
                {
                    "role": "user", 
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": "Analyze this image and describe what you see, focusing on any people, their emotions, and the overall situation."}
                    ]
                }
            ]
            
            # Aplicar template de chat
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text], images=[image], return_tensors="pt")
            
        elif "paligemma" in config['name'].lower():
            # PaliGemma usa formato directo
            prompt = "describe en detalle"
            inputs = processor(text=prompt, images=image, return_tensors="pt")
        
        else:
            # Formato genÃ©rico
            prompt = "Describe this image in detail"
            inputs = processor(text=prompt, images=image, return_tensors="pt")
        
        # Generar respuesta
        print("    âš¡ Generando descripciÃ³n de prueba...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=50,  # Limitado para la prueba
                num_beams=2,
                early_stopping=True,
                do_sample=False
            )
        
        # Decodificar segÃºn el modelo
        if "qwen" in config['name'].lower():
            # Para Qwen, extraer solo la parte nueva
            input_len = inputs["input_ids"].shape[1]
            generated_ids = generated_ids[:, input_len:]
            description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        else:
            # Para otros modelos
            description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"    ðŸ“ Respuesta: {description[:100]}...")
        print(f"    âœ… {config['name']} funcionando correctamente")
        
    except Exception as e:
        print(f"    âš ï¸ Error en prueba: {str(e)}")
        print(f"    ðŸ’¡ El modelo se instalÃ³ pero la prueba fallÃ³. Esto es normal para algunos modelos nuevos.")

def create_latest_prompts():
    """Crea prompts optimizados para Gemma 3 y Qwen2.5-VL"""
    latest_prompts = {
        # Prompts para Qwen2.5-VL (formato chat)
        "qwen25_figure_analysis": {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Analyze the main human figure in this image. Provide a detailed description of their physical appearance, emotional state, body language, clothing, and the situation they find themselves in. Pay attention to facial expressions, posture, and any contextual clues about their circumstances."}
            ]
        },
        
        "qwen25_emotional_deep": {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Focus deeply on the emotional and psychological aspects of the person in this image. What emotions are visible? What does their body language suggest about their mental state? What situation might have led to this emotional expression?"}
            ]
        },
        
        "qwen25_comprehensive": {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "Provide the most comprehensive analysis possible of this image, focusing particularly on any human subjects. Include physical appearance, emotional state, activities, environment, clothing, facial expressions, body language, and any contextual information that helps understand the scene and the person's situation."}
            ]
        },
        
        # Prompts para PaliGemma (formato directo)
        "paligemma_detailed": "Describe en gran detalle esta imagen, enfocÃ¡ndote en las personas, sus emociones, actividades y el contexto de la situaciÃ³n",
        
        "paligemma_figure": "Analiza la figura principal en esta imagen: apariencia fÃ­sica, expresiÃ³n facial, lenguaje corporal, ropa y situaciÃ³n",
        
        "paligemma_emotional": "Describe el estado emocional y psicolÃ³gico de la persona en esta imagen, incluyendo expresiones faciales y lenguaje corporal",
        
        # Prompts genÃ©ricos optimizados para modelos nuevos
        "latest_comprehensive": "Provide an extremely detailed and comprehensive analysis of this image, with special focus on any human figures, their emotional states, activities, and the overall context of the scene.",
        
        "latest_figure_focus": "Focus specifically on analyzing any human figures in this image. Describe their appearance, emotions, actions, and the situation they appear to be in.",
        
        "latest_situational": "Analyze the situation depicted in this image. What is happening? Who are the people involved? What is the context and what might have led to this moment?"
    }
    
    # Guardar prompts
    with open("latest_prompts.py", "w", encoding="utf-8") as f:
        f.write("# Prompts optimizados para Gemma 3 y Qwen2.5-VL\n\n")
        f.write("import json\n\n")
        f.write("LATEST_PROMPTS = {\n")
        
        for key, prompt in latest_prompts.items():
            if isinstance(prompt, dict):
                # Para prompts de chat (Qwen)
                f.write(f'    "{key}": {json.dumps(prompt, ensure_ascii=False, indent=8)},\n\n')
            else:
                # Para prompts de texto directo
                f.write(f'    "{key}": """{prompt}""",\n\n')
        
        f.write("}\n\n")
        
        f.write("""
def get_qwen25_prompt(prompt_type, custom_text=""):
    \"\"\"Genera prompt para Qwen2.5-VL en formato chat\"\"\"
    base_prompts = {
        'figure': LATEST_PROMPTS['qwen25_figure_analysis'],
        'emotional': LATEST_PROMPTS['qwen25_emotional_deep'],
        'comprehensive': LATEST_PROMPTS['qwen25_comprehensive']
    }
    
    if custom_text:
        return {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": custom_text}
            ]
        }
    
    return base_prompts.get(prompt_type, base_prompts['figure'])

def get_paligemma_prompt(prompt_type, custom_text=""):
    \"\"\"Genera prompt para PaliGemma\"\"\"
    base_prompts = {
        'figure': LATEST_PROMPTS['paligemma_figure'],
        'emotional': LATEST_PROMPTS['paligemma_emotional'],
        'detailed': LATEST_PROMPTS['paligemma_detailed']
    }
    
    if custom_text:
        return custom_text
    
    return base_prompts.get(prompt_type, base_prompts['detailed'])

# Ejemplo de uso:
# from latest_prompts import get_qwen25_prompt, get_paligemma_prompt
# qwen_prompt = get_qwen25_prompt('figure')
# gemma_prompt = get_paligemma_prompt('emotional')
""")
    
    print("ðŸ“š Prompts optimizados guardados en 'latest_prompts.py'")

if __name__ == "__main__":
    print("ðŸŽ¯ Setup para MODELOS DE ÃšLTIMA GENERACIÃ“N")
    print("ðŸ”§ Gemma 3 (PaliGemma) + Qwen2.5-VL")
    print("ðŸ’» Hardware: AMD 6750XT + Ryzen 7 5700X + 32GB RAM")
    print("=" * 70)
    
    try:
        setup_latest_models()
        create_latest_prompts()
        
        print("\nðŸŽ‰ Â¡ConfiguraciÃ³n de modelos de Ãºltima generaciÃ³n completada!")
        print("ðŸ“‹ PrÃ³ximos pasos:")
        print("  1. Usar server_latest.py para cargar estos modelos")
        print("  2. Probar con client_complete.py")
        print("  3. Experimentar con latest_prompts.py")
        
        print("\nðŸ’¡ Modelos recomendados para anÃ¡lisis de figuras:")
        print("  ðŸ¥‡ Qwen2.5-VL-7B - Estado del arte en comprensiÃ³n visual")
        print("  ðŸ¥ˆ PaliGemma-3B-Mix - Excelente de Google Research")
        print("  ðŸ¥‰ Qwen2.5-VL-72B-AWQ - MÃ¡xima calidad posible")
        
        print("\nâš ï¸ Nota importante:")
        print("  Estos son modelos muy nuevos (2024). Si alguno falla:")
        print("  â€¢ Actualiza transformers: pip install transformers>=4.44.0")
        print("  â€¢ Verifica dependencias: pip install qwen-vl accelerate")
        
    except Exception as e:
        print(f"âŒ Error durante la configuraciÃ³n: {str(e)}")
        print("ðŸ’¡ Soluciones sugeridas:")
        print("  â€¢ pip install --upgrade transformers accelerate torch")
        print("  â€¢ Verificar conexiÃ³n a internet estable")
        print("  â€¢ Comprobar espacio disponible en disco (>20GB)")